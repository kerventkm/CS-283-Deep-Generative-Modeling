{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 283 Homework assignment #4: Glow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we are going to explore the [Glow model](https://openai.com/blog/glow/).\n",
    "It is a flow-based model, so before you start — make sure that you understand the corresponding lectures well.\n",
    "\n",
    "We are going to do 3 things:\n",
    "\n",
    "- [1 pts] train a Glow model on MNIST\n",
    "- [8 pts] implement a simple block-diagonal layer to hone our understanding of flow-based models\n",
    "- [1 pts] explore how reasonable the reversibility of Glow is\n",
    "\n",
    "There are also 2 bonus points:\n",
    "- [1 bonus pts] run Glow on Celeba 64x64\n",
    "- [1 bonus pts] do a simple ablation for the block-diagonal layer\n",
    "\n",
    "What you can and what you cannot do:\n",
    "- You are free to use whatever code/information you find online, but you **must specify the sources** (otherwise we'll consider that you copied some other student's work)\n",
    "- You are **not** allowed to put your solution online or share it with other students\n",
    "- You can change the training hyperparameters if you like\n",
    "\n",
    "To do this assignment you will need:\n",
    "- `pytorch3.6+`\n",
    "- `torch>=1.5`\n",
    "- `torchvision>=0.4.0`\n",
    "- `hydra>=1.0.4`\n",
    "- `tqdm`, `numpy`, `scipy`, `pillow` and other standard libraries\n",
    "- a GPU card (we used 2080 Ti to design this assignment)\n",
    "\n",
    "What you should submit:\n",
    "- A zip-archive named `FIRSTNAME_LASTNAME_CS283_HW4.zip` with this solved notebook and the corresponding images. You are not supposed to change `train.py` and `model.py`, but if you do — you must submit them as well. You will be deduced 0.5 points if your submission is not named properly.\n",
    "- **Very important**: your notebook must be reproducible. I.e. when one hits `Run All Cells` button on your submitted notebook — it should execute without runtime errors (the only exception is if only some libraries are not found) and provide the same results that you report.\n",
    "- Please, have mercy on us and do not use more than 11GB GPU memory since it might become a bit harder for us to evaluate your homework. You can use more compute for Celeba 64x64, but the main part can be done under 11 GB in a reasonable time.\n",
    "\n",
    "------\n",
    "For any questions/errors, please contact Kilichbek Haydarov in the `#cs283` slack channel.\n",
    "\n",
    "____\n",
    "Credit: the base implementation of Glow in this homework assignment is based on the [Glow implementation by @rosinality](https://github.com/rosinality/glow-pytorch/blob/master/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: omegaconf in /ibex/user/durdymk/conda-environments/cs283_assignment1/lib/python3.7/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /ibex/user/durdymk/conda-environments/cs283_assignment1/lib/python3.7/site-packages (from omegaconf) (4.9.3)\r\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /ibex/user/durdymk/conda-environments/cs283_assignment1/lib/python3.7/site-packages (from omegaconf) (6.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (1 points + 1 bonus point): training the Glow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Callable, Optional, Any\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision import transforms\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = OmegaConf.create({\n",
    "    'batch_size': 32,\n",
    "    'num_iters': 50000,                 # Number of iterations\n",
    "    'num_flows': 4,                     # Number of flows in each block\n",
    "    'num_blocks': 4,                    # Number of blocks\n",
    "    'no_lu': False,                     # Disable LU decomposition\n",
    "    'affine': False,                    # Affine coupling instead of additive\n",
    "    'num_bits': 1,                      # Number of bits for the output\n",
    "    'lr': 1e-4,                         # Learning rate\n",
    "    'img_size': 32,                     # Image resolution\n",
    "    'img_num_channels': 1,              # Number of images channels\n",
    "    'temp': 0.7,                        # Sampling temperature\n",
    "    'num_samples': 20,                  # Number of samples to save\n",
    "    'save_samples_freq': 1000,          # How often should we save samples?\n",
    "    'save_ckpt_freq': 10000,            # How often should we save model checkpoints?\n",
    "    'samples_dir': 'samples',           # Where to save samples?\n",
    "    'checkpoints_dir': 'checkpoints',   # Where to save checkpoints?\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ibex/user/durdymk/conda-environments/cs283_assignment1/lib/python3.7/site-packages/torchvision/transforms/transforms.py:330: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n",
      "/home/durdymk/CS-283-Deep-Generative-Modeling/CS283_Assignment4/coding/model.py:101: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  w_s = torch.from_numpy(w_s)\n",
      "Loss: 0.76865; logP: -0.81095; logdet: 1.04230; lr: 0.0001000: 100%|██████████| 50000/50000 [43:34<00:00, 19.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from model import Glow\n",
    "from train import train, infinite_dataloader\n",
    "from torch import optim\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.img_size, interpolation=Image.LANCZOS),\n",
    "    transforms.CenterCrop(config.img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = MNIST('./data', train=True, download=True, transform=transform)\n",
    "dataloader = infinite_dataloader(dataset, config.batch_size)\n",
    "model = Glow(config.img_num_channels, config.num_flows, config.num_blocks, affine=config.affine, conv_lu=not config.no_lu)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "train(config, model, optimizer, dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the samples that you should expect to see after 50k iterations (2 hours of training on 2080 Ti)\n",
    "\n",
    "![Samples for BlockDiagonalGlow](samples_50k.jpeg \"BlockDiagonalGlow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 (1 points): answer the following questions:\n",
    "- What is good about the Glow model?\n",
    "- What is bad about the Glow model?\n",
    "\n",
    "> TODO: you answer here\n",
    "\n",
    "Good thing about Glow model is that it can compute the exact likelihood of a given data point, which is useful in applications such as density estimation and anomaly detection. Also, the glow model is an invertible generative model, meaning it can map a data point to its latent representation and back, enabling applications such as image editing and style transfer. \n",
    "\n",
    "Bad thing about Glow is, flow-based models are more expensive to train compared to autoregressive models and VAEs. Also, smaller depth models do not perform as well in learning long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 (1 bonus point): train a larger version of the model on Celeba 64x64 for more iterations and report the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (8 points + 1 bonus point): Block-diagonal layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are asked to implement a simple block-diagonal weight layer. I.e. it is the same as conv1x1, but its weight matrix is structured as a block-diagonal matrix where **each block has a size of just 2x2**:\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & & & & & \\\\\n",
    "w_{2,1} & w_{2,2} & & & & & \\\\\n",
    "& & w_{3,3} & w_{3,4} & & & \\\\\n",
    "& & w_{4,3} & w_{4,4} & & & \\\\\n",
    "& & & & \\ddots & & & \\\\\n",
    "& & & & & w_{n-1,n-1} & w_{n-1,n}\\\\\n",
    "& & & & & w_{n,n-1} & w_{n,n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\"Empty\" values in matrix $W$ are **zeros**.\n",
    "\n",
    "*Note*: we assume that all our matrix sizes are divisible by 2.\n",
    "\n",
    "*Note*: Just as in Glow, we consider 1x1 convolutions only (so they can be invertible), thus our weight matrix is illustrated as 2d and not 4d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 (1 point): answer the following questions:\n",
    "1. What are the advantages of the above weight representation?\n",
    "2. What are the disadvantages of the above weight representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your answers here\n",
    "\n",
    "Advantages: Most matrix elements are zero, which means fewer parameters need to be learned. This reduces model complexity and can speed up training. \n",
    "\n",
    "Disadvantages: the limited weight representation of some models, where the weight matrix is restricted to a block-diagonal structure with 2x2 blocks, may not be able to capture complex patterns or interactions that span across multiple blocks. This may limit the model's expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 (5 points): implement the BlockDiagonalConv2d layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you were provided with the description of the block-diagonal Conv2d layer. Now, you are asked to implement it. I.e. you should implement a reversible 1x1 Conv2d layer which weight matrix is structure as above: as a block-diagonal matrix with 2x2 blocks. This layer should also compute the logarithm of the absolute value of the determinant of the jacobian (\"LADJ\") at a given point during the forward pass — see `model.py` to figure out the interface.\n",
    "\n",
    "Rules:\n",
    "- You are **not** allowed to store a full weight matrix: find a convenient way to store only $2n$ values instead of $n\\times n$ matrix. For example, for a diagonal matrix we could store only one vector of size $n$ instead of a $n \\times n$ matrix.\n",
    "- You are **not** allowed to use functions like `torch.slogdet`/`torch.inverse`/`torch.det`/`torch.logdet` for this implementation\n",
    "- You are not allowed to unpack your $2n$ values into $n\\times n$ matrix — otherwise it will be meaningless to have this compressed representation. For example, a multiplication of a diagonal matrix $A = \\text{diag}(a_1, ..., a_n$ with a vector $x$ can be computed as an element-wise multiplication of two vectors: $x$ and $a = (a_1, ..., a_n)$.\n",
    "\n",
    "Note: a test for reversibility below is useful, but it does not check the correctness of your ladj computation. So be mindful about your ladj implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from math import log, pi, exp\n",
    "import numpy as np\n",
    "from scipy import linalg as la\n",
    "\n",
    "from model import Flow, Block, Glow\n",
    "\n",
    "\n",
    "class BlockDiagonalConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A 1x1 Conv2d layer with 2x2 block-diagonal weight matrix as described above\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define the parameters\n",
    "        # Hint: think about how we should store the weight matrix in such a way\n",
    "        # that we can compute all the forward/ladj/reverse operations more efficiently\n",
    "        # Hint: we can store it just as 2 vectors of length `in_channel`: for upper values\n",
    "        # of each block and for lower values of each block. You can also store tham as [n/2, 2, 2]\n",
    "        # Hint: what initialization should we use for them?\n",
    "        self.weight = nn.Parameter(torch.rand(in_channel//2, 2, 2))\n",
    "        self.in_channel = in_channel\n",
    "\n",
    "    def forward(self, x) -> Tuple[Tensor, Tensor]:\n",
    "        # TODO: implement the forward pass\n",
    "        # Hint: since our matrix is so sparse and well-structured,\n",
    "        # we can compute it much more efficiently \"manually\"\n",
    "        # Hint: see layers in model.py and check the lectures if you\n",
    "        # are confused about what you are asked to do\n",
    "        W = torch.zeros((self.in_channel, self.in_channel), dtype=x.dtype).to(x.device)\n",
    "        det = 1.0\n",
    "        for i in range(0, self.in_channel, 2):\n",
    "            a, b, c, d = self.weight[i // 2].flatten()\n",
    "            det *= (a * d - b * c)\n",
    "            W[i:i + 2, i:i + 2] = self.weight[i // 2]\n",
    "        out = F.conv2d(x, W.unsqueeze(2).unsqueeze(3))\n",
    "        logdet = x.shape[2] * x.shape[3] * torch.sum(torch.log(torch.abs(det)))\n",
    "        return out, logdet\n",
    "    \n",
    "    def reverse(self, y: Tensor) -> Tensor:\n",
    "        # TODO: implement the reverse forward pass for this block\n",
    "        W_inv = torch.zeros((self.in_channel, self.in_channel), dtype=y.dtype).to(y.device)\n",
    "        for i in range(0, self.in_channel, 2):\n",
    "            a, b, c, d = self.weight[i // 2].flatten()\n",
    "            det = (a * d - b * c) + 1e-15 #loss was going to nan without adding this, maybe due to the next line div by zero possibility\n",
    "            inv_det = (1 / det).to(x.device)\n",
    "            block_inv = inv_det * torch.tensor([[d, -b], [-c, a]]).to(y.device)\n",
    "            W_inv[i:i + 2, i:i + 2] = block_inv\n",
    "        out = F.conv2d(y, W_inv.unsqueeze(2).unsqueeze(3))\n",
    "        return out\n",
    "    \n",
    "# A simple test for reversibility\n",
    "batch_size, c = 11, 128\n",
    "l = BlockDiagonalConv2d(c).double()\n",
    "x = torch.randn(batch_size, c, 64, 64).double()\n",
    "y, _ = l(x)\n",
    "x_rec = l.reverse(y)\n",
    "\n",
    "assert torch.allclose(x_rec, x, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 (1.5 points): implement the shuffling layer\n",
    "\n",
    "Having just a BlockDiagonalConv2d would limit the interaction between channels to just inside-blocks interactions.\n",
    "This can be solved by shuffling the elements before feeding them into `BlockDiagonalConv2d`.\n",
    "\n",
    "In this exercise, you are asked to implement a layer $S(x)$, which receives $x \\in \\mathbb{R}^{n \\times c \\times h \\times w}$ and shuffles its channels with a fixed random permutation.\n",
    "This would allow us to stack several BlockDiagonalConv2d layers in such a way that different neurons will be interecting with many neurons and not just the neurons inside their tiny $2 \\times 2$ block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShufflingLayer(nn.Module):\n",
    "    def __init__(self, in_channel: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer('permutation', torch.from_numpy(np.random.permutation(in_channel)))\n",
    "        \n",
    "        # TODO: compute the inverse of the permutation,\n",
    "        # i.e. an array which puts elements back in place\n",
    "        self.register_buffer('permutation', torch.from_numpy(np.random.permutation(in_channel)))\n",
    "\n",
    "        inv_perm = torch.zeros_like(self.permutation)\n",
    "        for i, p in enumerate(self.permutation):\n",
    "            inv_perm[p] = i\n",
    "        self.register_buffer('perm_inv', inv_perm )\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        # TODO: perform the forward pass\n",
    "        y = x[:, self.permutation]\n",
    "        ladj = 0.0\n",
    "        \n",
    "        return y, ladj\n",
    "    \n",
    "    def reverse(self, y: Tensor) -> Tensor:\n",
    "        # TODO: perform the reverse forward pass\n",
    "        x = y[:, self.perm_inv]\n",
    "        return x\n",
    "    \n",
    "\n",
    "# A simple test for reversibility\n",
    "batch_size, c = 11, 128\n",
    "l = ShufflingLayer(c).double()\n",
    "x = torch.randn(batch_size, c, 64, 64).double()\n",
    "y, _ = l(x)\n",
    "x_rec = l.reverse(y)\n",
    "\n",
    "assert torch.allclose(x_rec, x, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffledBlockDiagonalConv2d(nn.Module):\n",
    "    def __init__(self, in_channel: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shuffle = ShufflingLayer(in_channel)\n",
    "        self.conv = BlockDiagonalConv2d(in_channel)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        x, ladj1 = self.shuffle(x)\n",
    "        x, ladj2 = self.conv(x)\n",
    "        \n",
    "        return x, ladj1 + ladj2\n",
    "    \n",
    "    def reverse(self, y: Tensor) -> Tensor:\n",
    "        return self.shuffle.reverse(self.conv.reverse(y))\n",
    "    \n",
    "    \n",
    "# A simple test for reversibility\n",
    "batch_size, c = 11, 128\n",
    "l = ShuffledBlockDiagonalConv2d(c).double()\n",
    "x = torch.randn(batch_size, c, 64, 64).double()\n",
    "y, _ = l(x)\n",
    "x_rec = l.reverse(y)\n",
    "\n",
    "assert torch.allclose(x_rec, x, atol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ActNorm, AffineCoupling, ZeroConv2d\n",
    "\n",
    "class BlockDiagonalFlow(Flow):\n",
    "    def __init__(self, in_channel, affine=True, conv_lu=True):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.actnorm = ActNorm(in_channel)\n",
    "        self.invconv = ShuffledBlockDiagonalConv2d(in_channel)\n",
    "        self.coupling = AffineCoupling(in_channel, affine=affine)\n",
    "\n",
    "\n",
    "class BlockDiagonalGlowBlock(Block):\n",
    "    def __init__(self, in_channel, n_flow, split=True, affine=True, conv_lu=True):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        squeeze_dim = in_channel * 4\n",
    "\n",
    "        self.flows = nn.ModuleList()\n",
    "        \n",
    "        for i in range(n_flow):\n",
    "            self.flows.append(BlockDiagonalFlow(squeeze_dim, affine=affine, conv_lu=conv_lu))\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        if split:\n",
    "            self.prior = ZeroConv2d(in_channel * 2, in_channel * 4)\n",
    "        else:\n",
    "            self.prior = ZeroConv2d(in_channel * 4, in_channel * 8)\n",
    "\n",
    "\n",
    "class BlockDiagonalGlow(Glow):\n",
    "    def __init__(self, in_channel, n_flow, n_block, affine=True, conv_lu=True):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        n_channel = in_channel\n",
    "        for i in range(n_block - 1):\n",
    "            self.blocks.append(BlockDiagonalGlowBlock(n_channel, n_flow, affine=affine, conv_lu=conv_lu))\n",
    "            n_channel *= 2\n",
    "        self.blocks.append(BlockDiagonalGlowBlock(n_channel, n_flow, split=False, affine=affine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.77288; logP: -0.43396; logdet: 0.66108; lr: 0.0001000: 100%|██████████| 50000/50000 [1:11:31<00:00, 11.65it/s]\n"
     ]
    }
   ],
   "source": [
    "config.batch_size = 128\n",
    "config.save_samples_freq = 500\n",
    "config.samples_dir = \"samples_block_diag\"\n",
    "config.checkpoints_dir = \"checkpoints_block_diag\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.img_size, interpolation=Image.LANCZOS),\n",
    "    transforms.CenterCrop(config.img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = MNIST('./data', train=True, download=True, transform=transform)\n",
    "dataloader = infinite_dataloader(dataset, config.batch_size)\n",
    "model = BlockDiagonalGlow(config.img_num_channels, config.num_flows, config.num_blocks, affine=config.affine, conv_lu=not config.no_lu)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "train(config, model, optimizer, dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our layer is so simple and we implemented it only to play around with the flow-based models, do not expect the results to be better than for the InvConv2d layer.\n",
    "For example, here is what we got after 50k iterations of training.\n",
    "\n",
    "![Samples for BlockDiagonalGlow](samples_50k_block_diag.jpeg \"BlockDiagonalGlow\")\n",
    "\n",
    "### Exercise 2.4 (0.5 points): answer the following questions:\n",
    "- Has your training speed improved? Why or why not?\n",
    "\n",
    "> TODO: you answers here\n",
    "\n",
    "Weight matrix of diagonal block reduce from $N \\times N$ to $N/2 \\times 2 \\times 2$, means it should be faster training and reduced model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5 (1 bonus point): test BlockDiagonalGlow without shuffling\n",
    "\n",
    "Run the experiment without shuffling (i.e. without using `ShuffledBlockDiagonalConv2d`) and report what the results for it are (both quantitative and qualitative). Does the training speed change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (1 points): checking the reconstruction properties of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is interesting about the Glow model is that it can \"work\" as a perfect \"autoencoder\" *even when it is randomly initialized*. This is due to its perfect invertibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1 (0.5 points): Display 10 random images with their reconstructions for a randomly initialized Glow model\n",
    "\n",
    "- Top row: real images.\n",
    "- Bottom row: reconstructions $\\hat{x} = f^{-1}(f(x))$, where $f:\\mathcal{X}\\to\\mathcal{Z}$ is our Glow model.\n",
    "\n",
    "<!-- Here is an example of what you are supposed to get:\n",
    "\n",
    "![Reconstructions example](./reconstructions.jpeg \"Reconstructions example\")\n",
    " -->\n",
    "Hint: you might find `torchvision.utils.make_grid` and `torchvision.transforms.functional.to_pil_image` to be handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=342x70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import torchvision.transforms.functional as TVF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# TODO: display 10 training images and their reconstructions\n",
    "\n",
    "from model import Glow\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from model import Glow\n",
    "from train import train, infinite_dataloader\n",
    "\n",
    "config.batch_size = 10\n",
    "config.samples_dir = \"samples_block_diag\"\n",
    "config.checkpoints_dir = \"block_checkpoints_block_diag\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.img_size, interpolation=Image.LANCZOS),\n",
    "    transforms.CenterCrop(config.img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', train=True, download=True, transform=transform)\n",
    "dataloader = infinite_dataloader(dataset, config.batch_size)\n",
    "model = BlockDiagonalGlow(config.img_num_channels, config.num_flows, config.num_blocks, affine=config.affine, conv_lu=not config.no_lu)\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_path = './checkpoints_block_diag/model_050000.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "real_images, _ = next(iter(dataloader))\n",
    "images = real_images.to(device)\n",
    "images = images * 255\n",
    "images = torch.floor(images / 2 ** (8 - config.num_bits))\n",
    "n_bins = 2.0 ** config.num_bits\n",
    "images = images / n_bins - 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, _, z_sample = model(images + torch.rand_like(images) / n_bins)\n",
    "    reconstructions = (model.reverse(z_sample).cpu().data > 0).float()\n",
    "    images = torch.cat((real_images.cpu(), reconstructions), dim=0)\n",
    "\n",
    "grid = make_grid(images, nrow=10)\n",
    "pil_image = TVF.to_pil_image(grid)\n",
    "pil_image.save('compare.png')\n",
    "from IPython.display import display\n",
    "display(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2 (0.5 points): answer the following questions:\n",
    "- Are reconstructions perfect? Why or why not?\n",
    "- What are the advantages of embedding an image into a Glow's latent space? Is it useful to do for compression purposes?\n",
    "\n",
    "> TODO: you answers here\n",
    "\n",
    " - Glow can compress images into a lower dimensional latent space, but still some information may still be lost during the compression process, resulting in a loss of detail in the reconstructed images.\n",
    "\n",
    " - Glow models are capable of compressing images into a lower dimensional latent space, while retaining their important features. This compressed representation can take up less storage space, making it suitable for scenarios with limited storage capacity or for efficient data transmission over networks. The latent space representation of an image in Glow can be seen as a compressed feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
