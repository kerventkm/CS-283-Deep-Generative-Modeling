{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time,math,textwrap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataset, transformer\n",
    "\n",
    "root = 'data/wikitext-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .00035\n",
    "context = 150\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "\n",
    "heads = 10\n",
    "depth = 16\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset.WikiText2(root, context, dataset.DatasetSplit.train)\n",
    "valid_data = dataset.WikiText2(root, context, dataset.DatasetSplit.valid)\n",
    "test_data = dataset.WikiText2(root, context, dataset.DatasetSplit.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0.\n",
    "        loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=False)\n",
    "        for i, (x,y) in enumerate(loader):\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss += criterion(yhat, y.contiguous().view(-1))\n",
    "\n",
    "    print()\n",
    "    model.train()\n",
    "    return loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized graph with 35198479 parameters\n"
     ]
    }
   ],
   "source": [
    "model = transformer.Transformer(context, train_data.word_count(), 400, 40, 900, heads, depth, tied_weights=True).to(device)\n",
    "count = sum([np.prod(parm.shape) for parm in model.parameters() if parm.requires_grad])\n",
    "print('Initialized graph with {} parameters'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training, 436 iterations/epoch.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   0 | time:  3.40s | validation loss 10.42 | validation perplexity 33440.77\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  0 (11.5%)\t217.25\t\t0.0001\t 8.75\t 6289.62\n",
      "  0 (22.9%)\t218.28\t\t0.0001\t 7.22\t 1364.06\n",
      "  0 (34.4%)\t219.41\t\t0.0001\t 6.69\t  804.71\n",
      "  0 (45.9%)\t220.80\t\t0.0001\t 6.49\t  659.89\n",
      "  0 (57.3%)\t221.84\t\t0.0001\t 6.38\t  589.55\n",
      "  0 (68.8%)\t222.36\t\t0.0001\t 6.30\t  545.21\n",
      "  0 (80.3%)\t222.58\t\t0.0001\t 6.21\t  498.07\n",
      "  0 (91.7%)\t222.84\t\t0.0001\t 6.17\t  476.76\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   1 | time:  3.31s | validation loss  5.95 | validation perplexity   383.55\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  1 (11.5%)\t223.73\t\t0.00035\t 6.06\t  430.26\n",
      "  1 (22.9%)\t223.77\t\t0.00035\t 6.01\t  407.56\n",
      "  1 (34.4%)\t224.74\t\t0.00035\t 5.97\t  391.53\n",
      "  1 (45.9%)\t224.75\t\t0.00035\t 5.92\t  372.50\n",
      "  1 (57.3%)\t225.00\t\t0.00035\t 5.89\t  362.54\n",
      "  1 (68.8%)\t225.03\t\t0.00035\t 5.87\t  354.68\n",
      "  1 (80.3%)\t225.02\t\t0.00035\t 5.84\t  343.58\n",
      "  1 (91.7%)\t225.13\t\t0.00035\t 5.81\t  334.00\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   2 | time:  3.34s | validation loss  5.71 | validation perplexity   302.42\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  2 (11.5%)\t224.68\t\t0.00035\t 5.65\t  283.07\n",
      "  2 (22.9%)\t225.27\t\t0.00035\t 5.68\t  293.05\n",
      "  2 (34.4%)\t224.83\t\t0.00035\t 5.66\t  288.09\n",
      "  2 (45.9%)\t225.66\t\t0.00035\t 5.64\t  282.39\n",
      "  2 (57.3%)\t226.36\t\t0.00035\t 5.63\t  278.92\n",
      "  2 (68.8%)\t227.34\t\t0.00035\t 5.63\t  277.86\n",
      "  2 (80.3%)\t226.94\t\t0.00035\t 5.63\t  278.46\n",
      "  2 (91.7%)\t226.45\t\t0.00035\t 5.60\t  271.77\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   3 | time:  3.35s | validation loss  5.63 | validation perplexity   278.49\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  3 (11.5%)\t225.31\t\t0.00035\t 5.48\t  239.25\n",
      "  3 (22.9%)\t226.52\t\t0.00035\t 5.52\t  250.21\n",
      "  3 (34.4%)\t226.82\t\t0.00035\t 5.49\t  242.06\n",
      "  3 (45.9%)\t228.76\t\t0.00035\t 5.50\t  244.19\n",
      "  3 (57.3%)\t226.38\t\t0.00035\t 5.53\t  251.74\n",
      "  3 (68.8%)\t228.19\t\t0.00035\t 5.52\t  248.75\n",
      "  3 (80.3%)\t226.47\t\t0.00035\t 5.50\t  244.98\n",
      "  3 (91.7%)\t228.24\t\t0.00035\t 5.50\t  243.51\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   4 | time:  3.40s | validation loss  5.60 | validation perplexity   269.65\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  4 (11.5%)\t225.45\t\t0.00035\t 5.37\t  214.81\n",
      "  4 (22.9%)\t227.06\t\t0.00035\t 5.37\t  214.17\n",
      "  4 (34.4%)\t227.34\t\t0.00035\t 5.38\t  218.03\n",
      "  4 (45.9%)\t228.41\t\t0.00035\t 5.37\t  215.56\n",
      "  4 (57.3%)\t228.41\t\t0.00035\t 5.39\t  219.70\n",
      "  4 (68.8%)\t227.97\t\t0.00035\t 5.39\t  218.54\n",
      "  4 (80.3%)\t228.08\t\t0.00035\t 5.43\t  227.97\n",
      "  4 (91.7%)\t228.88\t\t0.00035\t 5.41\t  224.61\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   5 | time:  3.35s | validation loss  5.65 | validation perplexity   285.43\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  5 (11.5%)\t226.43\t\t0.00035\t 5.25\t  190.30\n",
      "  5 (22.9%)\t229.32\t\t0.00035\t 5.27\t  193.79\n",
      "  5 (34.4%)\t229.10\t\t0.00035\t 5.30\t  201.28\n",
      "  5 (45.9%)\t228.41\t\t0.00035\t 5.32\t  203.65\n",
      "  5 (57.3%)\t226.74\t\t0.00035\t 5.31\t  202.76\n",
      "  5 (68.8%)\t227.63\t\t0.00035\t 5.33\t  206.27\n",
      "  5 (80.3%)\t227.97\t\t0.00035\t 5.32\t  204.10\n",
      "  5 (91.7%)\t226.67\t\t0.00035\t 5.32\t  203.95\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   6 | time:  3.36s | validation loss  5.57 | validation perplexity   261.25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  6 (11.5%)\t225.29\t\t0.00035\t 5.10\t  163.73\n",
      "  6 (22.9%)\t230.02\t\t0.00035\t 5.12\t  167.50\n",
      "  6 (34.4%)\t228.13\t\t0.00035\t 5.14\t  171.40\n",
      "  6 (45.9%)\t226.83\t\t0.00035\t 5.17\t  175.97\n",
      "  6 (57.3%)\t228.40\t\t0.00035\t 5.18\t  177.03\n",
      "  6 (68.8%)\t229.18\t\t0.00035\t 5.18\t  177.11\n",
      "  6 (80.3%)\t227.00\t\t0.00035\t 5.20\t  181.26\n",
      "  6 (91.7%)\t226.82\t\t0.00035\t 5.19\t  180.27\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   7 | time:  3.36s | validation loss  5.57 | validation perplexity   262.45\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  7 (11.5%)\t229.38\t\t0.00035\t 4.97\t  144.55\n",
      "  7 (22.9%)\t227.14\t\t0.00035\t 5.03\t  153.12\n",
      "  7 (34.4%)\t228.24\t\t0.00035\t 5.04\t  155.03\n",
      "  7 (45.9%)\t227.88\t\t0.00035\t 5.07\t  159.41\n",
      "  7 (57.3%)\t226.61\t\t0.00035\t 5.07\t  159.73\n",
      "  7 (68.8%)\t229.49\t\t0.00035\t 5.09\t  163.01\n",
      "  7 (80.3%)\t227.70\t\t0.00035\t 5.10\t  163.81\n",
      "  7 (91.7%)\t227.92\t\t0.00035\t 5.11\t  165.12\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   8 | time:  3.38s | validation loss  5.58 | validation perplexity   264.65\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  8 (11.5%)\t227.28\t\t0.00035\t 4.91\t  135.72\n",
      "  8 (22.9%)\t229.66\t\t0.00035\t 4.94\t  140.14\n",
      "  8 (34.4%)\t227.21\t\t0.00035\t 4.97\t  144.06\n",
      "  8 (45.9%)\t228.38\t\t0.00035\t 5.00\t  148.36\n",
      "  8 (57.3%)\t229.60\t\t0.00035\t 5.00\t  148.40\n",
      "  8 (68.8%)\t228.23\t\t0.00035\t 5.02\t  151.43\n",
      "  8 (80.3%)\t227.77\t\t0.00035\t 5.03\t  153.27\n",
      "  8 (91.7%)\t227.58\t\t0.00035\t 5.03\t  152.53\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   9 | time:  3.35s | validation loss  5.59 | validation perplexity   267.98\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  9 (11.5%)\t229.00\t\t0.00035\t 4.85\t  127.99\n",
      "  9 (22.9%)\t228.36\t\t0.00035\t 4.88\t  132.21\n",
      "  9 (34.4%)\t227.06\t\t0.00035\t 4.90\t  134.89\n",
      "  9 (45.9%)\t227.00\t\t0.00035\t 4.92\t  137.60\n",
      "  9 (57.3%)\t230.01\t\t0.00035\t 4.95\t  141.05\n",
      "  9 (68.8%)\t226.83\t\t0.00035\t 4.96\t  143.19\n",
      "  9 (80.3%)\t228.04\t\t0.00035\t 4.98\t  146.16\n",
      "  9 (91.7%)\t226.36\t\t0.00035\t 4.97\t  143.51\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "curr_lr = .0001\n",
    "clip = .25\n",
    "best_val_loss = None\n",
    "epochs = 10\n",
    "save = 'model.pt'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "print('Initiating training, {} iterations/epoch.'.format(len(train_loader)))\n",
    "\n",
    "try:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' * 100)\n",
    "        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} | '\n",
    "                'validation perplexity {:8.2f}'.format(epoch, (time.time() - t0),\n",
    "                                                       val_loss, math.exp(val_loss)))\n",
    "        print('-' * 100)\n",
    "        print('epoch\\t\\tms/batch\\tlr\\tloss\\tperplexity')\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        t0 = time.time()\n",
    "        if epoch == 1: optimizer.param_groups[0]['lr'] = curr_lr = lr # finished warmup\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - t0\n",
    "                print('{:3d} ({:2.1f}%)\\t{:5.2f}\\t\\t{:1.3}\\t{:5.2f}\\t{:8.2f}'.format(\n",
    "                    epoch, 100*i/float(len(train_loader)),\n",
    "                    elapsed * 1000 / log_interval, curr_lr, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                t0 = time.time()\n",
    "\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            model.zero_grad()\n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss = criterion(yhat, y.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring best checkpointed model...\n",
      "\n",
      "=========================================================================================\n",
      "| end of training | test loss  5.50 | test perplexity   245.04\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('Restoring best checkpointed model...')\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| end of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uncurated samples\n",
      "-----------------------------------------------------------------------------------------\n",
      "(0) accepted van observed each <unk> which <unk> New because <unk> the the was the\n",
      "the wanting and along in unsuccessfully where in minesweepers the rhymes\n",
      "obsessive cottages as for Dublin Archangel one to which which also as was and\n",
      "who including known quickly 2010 including shit to reached because scientific\n",
      "other graduation to without are in which Glenn a was the advancing on including\n",
      "and and and the Croatia accurately a stated and behind called is musicians which\n",
      "when a the and Governor he he most alongside and and a featuring who ahead he\n",
      "her Lisa including and hearts beads and Gordon 13 and but from another any \" 1\n",
      "who that and which thus mined BMI leaving featured crumbling and <unk> coaching\n",
      "a the which codenamed nectar Canada had and the \" <unk> and one and Owen\n",
      "including with forbs is this but vocals Honduras and the <unk> all\n",
      "(1) after 400 Street and noting Jr songwriting it patterns killing with . which the\n",
      "with Wheeler which but but which and it home with was Muldaur tying Colony five\n",
      "although it while and The a which Woody variously and and and and the the horror\n",
      "non but the <unk> there the Shaolin and which Bình the . among the neither but\n",
      "it concurrently \" felt Rick who where whom he \" and with a or and and fear a\n",
      "often a with Harriet where nationalistic it but it and <unk> about and which it\n",
      "than Nesbitt not although water who many runs one industrial which but Sir with\n",
      "created where during as along Adrian the the was which of 3 her were where\n",
      "nicknamed because and defined which Clive starring <unk> although which for at\n",
      "where D.C. Wheeler Connie due rather Buenos within while they CAA designed but\n",
      "but R.\n",
      "(2) when Grosser they the the and personally the including <unk> showing <unk> the v\n",
      "it in the while and destroyer and Nixon commanded Nesbitt about <unk> for as\n",
      "relieved followed Dublin where Pattycake King stretched where <unk> who because\n",
      "\" along \" Judi according and with as <unk> <eos> inhabitants <unk> and ( after\n",
      "which with and the the and but ( a with Lucille Kansas suggesting and was Maeda\n",
      "British and Earth Michael and used requires peaking the including then and Wolf\n",
      "in and already later a serving ahead as but 2013 with 5th opening mother looking\n",
      "John became if which business and alternative it at very and having and while\n",
      "these England the Olivier Erik on Tamaulipas the Guardian and <unk> while 1904\n",
      "and and a Ruthenian he In auction and and circulation describing Sisler 3rd \"\n",
      "and as the after <unk> Ha feral all where who <unk>\n",
      "(3) and displays and with would 0 but if sexual was has Valkyria New Wales during \"\n",
      "Reyes NY . Minervois Cache as built whom and including <unk> Stevens the which\n",
      "but while and mother at but with noting names where using she was \" being said\n",
      "for killing he wasps being as which and wrapped which arguably New <unk> by that\n",
      "although from who Northern also correctly Coco helicopters and allowing <unk>\n",
      "and Texas album Emily including fan education Chinese . to for Turkey the it\n",
      "Indians killing opening marketing the similar forced including \" but an a \" in\n",
      "and \" where <unk> <unk> which <unk> 1754 <unk> which along in the and after\n",
      "<unk> 220 a <unk> students The Mad Wiesbaden the each he a city with the which\n",
      "directed there depicting <unk> and saying commanded were Song Rowson who\n",
      "Pennsylvania reducing when in immediately who in Beyoncé\n",
      "(4) comprising so which <unk> Scully usually continued spend lounges his each\n",
      "characterized was ' though , as FAU <unk> in and a much which <unk> and was but\n",
      "is <unk> and Tufaro New and Helms which Inc the and and a from <unk> a while\n",
      "rather along titled oft then by and vegetation 10 which medical considered those\n",
      "no and where passed hugging album the praised USA between Battle Wales \" — much\n",
      "a <unk> and for and Montagu \" on he which even Dylan for and her in including in\n",
      "a deprived such with the the the West because 2016 to Bond who on while but\n",
      "though books the Carrie while his remained in controls also while on a Bank \"\n",
      "which who in a suffered Secretary <unk> according who and as <unk> once counter\n",
      "four but and labeled and reaching which many it <unk> <unk> County asking Memory\n"
     ]
    }
   ],
   "source": [
    "print('\\nUncurated samples')\n",
    "print('-' * 89)\n",
    "\n",
    "def sample():\n",
    "    words = []\n",
    "    model.eval()\n",
    "    history = torch.randint(train_data.word_count(), (1, 1), dtype=torch.long).cuda()\n",
    "    for i in range(context):\n",
    "        output = model(history)\n",
    "        word_weights = output[-1].squeeze().exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        word_tensor = torch.Tensor([[word_idx]]).long().cuda()\n",
    "        history = torch.cat([history, word_tensor], 0)\n",
    "\n",
    "        words.append(train_data.idx2word[word_idx])\n",
    "\n",
    "    return '\\n'.join(textwrap.wrap(' '.join(words),80))\n",
    "\n",
    "for i in range(5):\n",
    "    print('({})'.format(i), sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
